{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd8aba",
   "metadata": {},
   "source": [
    "# CSC 8614 - Language Models\n",
    "## CI2 - Fine-tuning a language model for text classification\n",
    "\n",
    "In this TP, you will work on fine-tuning a language model to move from text generation to text classification, specifically working on Spam Detection.\n",
    "\n",
    "The exercise (and code) has been adapted from the book _Build a Large Language Model (From Scratch)_, by Sebastian Raschka, and its [official github repository](https://github.com/rasbt/LLMs-from-scratch).\n",
    "\n",
    "This TP will be done in this notebook, and requires some additional files (available from the course website). You will have to fill the missing portions of code, and perform some additional experiments by testing different parameters.\n",
    "\n",
    "Working on this TP:\n",
    "- The easiest way is probably to work directly on the notebook, using jupyter notebook or visual studio code. An alternative is also to use Google colab.\n",
    "- You should be able to run everything on your machine, but you can connect to the GPUs if needed.\n",
    "\n",
    "Some files are required, and are available on the course website:\n",
    "- `requirements.txt`\n",
    "- `gpt_utils.py`\n",
    "\n",
    "\n",
    "## About the report\n",
    "You will have to return this notebook (completed), as well as a mini-report (`TP2/rapport.md`).\n",
    "\n",
    "The notebook and report shall be submitted via a GitHub repository, similarly to what you did for the first session (remember to use a different folder: `TP2`).\n",
    "For the notebook, it is sufficient to complete the code and submit the final version.\n",
    "\n",
    "For the mini-report, you have to answer the questions asked in this notebook, and discuss some of your findings as requested.\n",
    "As for the first session:\n",
    "- \"Vous devez y mettre : réponses courtes, résultats observés (copie de sorties), captures d’écran demandées, et une courte interprétation.\"\n",
    "- \"Ne collez pas des pages entières : soyez concis et sélectionnez les éléments pertinents.\"\n",
    "\n",
    "Reproducibility: \n",
    "- fix a random seed and write it in the report\n",
    "- indicate in the report the specific python version OS, and the library versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e55fc",
   "metadata": {},
   "source": [
    "**Question 1**: Dans `TP1/rapport.md`, ajoutez immédiatement un court en-tête (quelques lignes) contenant : (i) votre nom/prénom, (ii) la commande d’installation/activation d’environnement utilisée, (iii) les versions (Python + bibliothèques principales).\n",
    "\n",
    "Ajoutez ensuite au fil du TP des sections/titres à votre convenance, tant que l’on peut retrouver clairement vos réponses et vos preuves d’exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af34f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.9.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Collecting tiktoken==0.12.0 (from -r requirements.txt (line 2))\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: pandas==2.3.3 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (2.3.3)\n",
      "Collecting matplotlib==3.10.8 (from -r requirements.txt (line 5))\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl.metadata (52 kB)\n",
      "Collecting tensorflow==2.20.0 (from -r requirements.txt (line 6))\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting jupyterlab==4.5.1 (from -r requirements.txt (line 7))\n",
      "  Downloading jupyterlab-4.5.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from tiktoken==0.12.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from tiktoken==0.12.0->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from tqdm==4.67.1->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2025.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.10.8->-r requirements.txt (line 5))\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.10.8->-r requirements.txt (line 5))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.10.8->-r requirements.txt (line 5))\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.10.8->-r requirements.txt (line 5))\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib==3.10.8->-r requirements.txt (line 5))\n",
      "  Downloading pillow-12.1.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib==3.10.8->-r requirements.txt (line 5))\n",
      "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading protobuf-6.33.4-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached wrapt-2.0.1-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached keras-3.13.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached ml_dtypes-0.5.4-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx<1,>=0.25.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.1.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.9.1)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.5.4)\n",
      "Requirement already satisfied: traitlets in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.14.3)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting anyio (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.8.19)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (9.9.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (8.8.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.2.1)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (27.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jinja2->torch==2.9.1->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jupyter-core->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.5.1)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jupyter_server_terminals-0.5.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting nbconvert>=6.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading prometheus_client-0.24.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pywinpty>=2.0.1 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading pywinpty-3.0.2-cp312-cp312-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading send2trash-2.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading json5-0.13.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached optree-0.18.0-cp312-cp312-win_amd64.whl.metadata (35 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.12.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.12.0->-r requirements.txt (line 2)) (2.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.9.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.6.3)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.25.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading rpds_py-0.30.0-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.0.3)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading bleach-6.3.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting defusedxml (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading mistune-3.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading nbclient-0.10.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.8.5)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading webcolors-25.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ahmed\\desktop\\csc8614\\venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.3)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading soupsieve-2.8.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading lark-1.3.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7))\n",
      "  Downloading arrow-1.4.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-win_amd64.whl (878 kB)\n",
      "   ---------------------------------------- 0.0/878.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 878.7/878.7 kB 38.5 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 83.8 MB/s eta 0:00:00\n",
      "Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "Downloading jupyterlab-4.5.1-py3-none-any.whl (12.4 MB)\n",
      "   ---------------------------------------- 0.0/12.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 12.4/12.4 MB 111.2 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 66.4 MB/s eta 0:00:00\n",
      "Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.7/4.7 MB 94.5 MB/s eta 0:00:00\n",
      "Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "Downloading jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Downloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
      "Using cached keras-3.13.0-py3-none-any.whl (1.5 MB)\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.5.4-cp312-cp312-win_amd64.whl (212 kB)\n",
      "Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pillow-12.1.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 7.0/7.0 MB 109.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-6.33.4-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 84.9 MB/s eta 0:00:00\n",
      "Using cached termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-2.0.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/10.2 MB 106.7 MB/s eta 0:00:00\n",
      "Downloading json5-0.13.0-py3-none-any.whl (36 kB)\n",
      "Downloading jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
      "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Downloading jupyter_server_terminals-0.5.4-py3-none-any.whl (13 kB)\n",
      "Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading prometheus_client-0.24.1-py3-none-any.whl (64 kB)\n",
      "Downloading pywinpty-3.0.2-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 57.7 MB/s eta 0:00:00\n",
      "Downloading send2trash-2.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.18.0-cp312-cp312-win_amd64.whl (312 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mistune-3.2.0-py3-none-any.whl (53 kB)\n",
      "Downloading nbclient-0.10.4-py3-none-any.whl (25 kB)\n",
      "Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading rpds_py-0.30.0-cp312-cp312-win_amd64.whl (240 kB)\n",
      "Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl (31 kB)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Downloading soupsieve-2.8.1-py3-none-any.whl (36 kB)\n",
      "Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.4.0-py3-none-any.whl (68 kB)\n",
      "Downloading lark-1.3.1-py3-none-any.whl (113 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: webencodings, namex, libclang, flatbuffers, fastjsonschema, wrapt, wheel, werkzeug, websocket-client, webcolors, uri-template, tinycss2, termcolor, tensorboard-data-server, soupsieve, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, pywinpty, python-json-logger, pyparsing, pycparser, protobuf, prometheus-client, pillow, pandocfilters, optree, opt_einsum, ml_dtypes, mistune, mdurl, markdown, lark, kiwisolver, jupyterlab-pygments, jsonpointer, json5, h5py, h11, grpcio, google_pasta, gast, fqdn, fonttools, defusedxml, cycler, contourpy, bleach, babel, attrs, async-lru, anyio, absl-py, tiktoken, terminado, tensorboard, rfc3987-syntax, referencing, matplotlib, markdown-it-py, httpcore, cffi, beautifulsoup4, astunparse, arrow, rich, jupyter-server-terminals, jsonschema-specifications, isoduration, httpx, argon2-cffi-bindings, keras, jsonschema, argon2-cffi, tensorflow, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab\n",
      "Successfully installed absl-py-2.3.1 anyio-4.12.1 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.4.0 astunparse-1.6.3 async-lru-2.0.5 attrs-25.4.0 babel-2.17.0 beautifulsoup4-4.14.3 bleach-6.3.0 cffi-2.0.0 contourpy-1.3.3 cycler-0.12.1 defusedxml-0.7.1 fastjsonschema-2.21.2 flatbuffers-25.12.19 fonttools-4.61.1 fqdn-1.5.1 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h11-0.16.0 h5py-3.15.1 httpcore-1.0.9 httpx-0.28.1 isoduration-20.11.0 json5-0.13.0 jsonpointer-3.0.0 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.4 jupyterlab-4.5.1 jupyterlab-pygments-0.3.0 jupyterlab-server-2.28.0 keras-3.13.0 kiwisolver-1.4.9 lark-1.3.1 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 matplotlib-3.10.8 mdurl-0.1.2 mistune-3.2.0 ml_dtypes-0.5.4 namex-0.1.0 nbclient-0.10.4 nbconvert-7.16.6 nbformat-5.10.4 notebook-shim-0.2.4 opt_einsum-3.4.0 optree-0.18.0 pandocfilters-1.5.1 pillow-12.1.0 prometheus-client-0.24.1 protobuf-6.33.4 pycparser-2.23 pyparsing-3.3.1 python-json-logger-4.0.0 pywinpty-3.0.2 referencing-0.37.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rich-14.2.0 rpds-py-0.30.0 send2trash-2.1.0 soupsieve-2.8.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 terminado-0.18.1 tiktoken-0.12.0 tinycss2-1.4.0 uri-template-1.3.0 webcolors-25.10.0 webencodings-0.5.1 websocket-client-1.9.0 werkzeug-3.1.5 wheel-0.45.1 wrapt-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# [Instructor code: install requirements]\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b498f1c",
   "metadata": {},
   "source": [
    "## Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6cd969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\Desktop\\CSC8614\\venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 14.4kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.52MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 89.6kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:14<00:00, 6.71MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 4.38MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 843kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 760kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights downloaded and loaded into memory.\n"
     ]
    }
   ],
   "source": [
    "# --- [INSTRUCTOR CODE: load the model weights into memory] ---\n",
    "import torch\n",
    "import tiktoken\n",
    "from gpt_utils import GPTModel, download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "# Download the model weights (124M param version) / This function (which we put in gpt_utils) handles the downloading\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2_weights\")\n",
    "print(\"Weights downloaded and loaded into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80dab0",
   "metadata": {},
   "source": [
    "The `settings` obtained with `download_and_load_gpt2` are the GPT-2 weights made publicly available by OpenAI.\n",
    "\n",
    "**Question 2**: What type is the object `setting`, and what is its structure (e.g. if it is a list, its length; if a dictionary, its keys, etc.)?\n",
    "\n",
    "**Question 3**: What type is the object `params`, and what is its structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6572fe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings type: <class 'dict'>\n",
      "Settings keys: dict_keys(['n_vocab', 'n_ctx', 'n_embd', 'n_head', 'n_layer'])\n",
      "Settings values: dict_values([50257, 1024, 768, 12, 12])\n",
      "Params type: <class 'dict'>\n",
      "Params keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "Shape of 'wpe': (1024, 768)\n",
      "Shape of first block's attention weights 'w': (768, 2304)\n"
     ]
    }
   ],
   "source": [
    "# Analyse `settings`\n",
    "print(\"Settings type:\", type(settings))\n",
    "print(\"Settings keys:\", settings.keys())\n",
    "print(\"Settings values:\", settings.values())\n",
    "\n",
    "\n",
    "# Analyse `params`\n",
    "print(\"Params type:\", type(params))\n",
    "print(\"Params keys:\", params.keys())\n",
    "print(\"Shape of 'wpe':\", params['wpe'].shape)\n",
    "print(\"Shape of first block's attention weights 'w':\", params['blocks'][0]['attn']['c_attn']['w'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aed6d2",
   "metadata": {},
   "source": [
    "Look at the `GPTModel` in the file `gpt_utils.py`. In the `__init__` method, we have to pass a config (parameter `cfg`). \n",
    "\n",
    "**Question 4:** \n",
    "Analyse the `__init__` method, and check what is the required structure for the `cfg` parameter. Is the `settings` variable we have obtained in the right format? If not, perform the mapping to convert the variable `setting` into a variable `model_config` with the right structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e344cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"vocab_size\": settings[\"n_vocab\"],\n",
    "    \"emb_dim\": settings[\"n_embd\"],\n",
    "    \"context_length\": settings[\"n_ctx\"],\n",
    "    \"n_layers\": settings[\"n_layer\"],\n",
    "    \"n_heads\": settings[\"n_head\"],\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6837eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Model Loaded and Configured successfully!\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(model_config)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval() \n",
    "\n",
    "print(\"GPT-2 Model Loaded and Configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb45e0b",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Context from the lecture: The raw data is just text messages. \n",
    "\n",
    "The model needs numbers (token IDs). We also need to pad the messages so they are all the same length in a batch.\n",
    "\n",
    "We will use a `SpamDataset` class (provided below) to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34fe9b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 5572\n",
      "Created 'train.csv' and 'test.csv' successfully!\n",
      "Train size: 4457\n",
      "Test size: 1115\n"
     ]
    }
   ],
   "source": [
    "# --- [INSTRUCTOR CODE: Run this cell to define the Dataset Class] ---\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=120, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = pad_token_id\n",
    "        # Encode labels: \"spam\" -> 1, \"ham\" -> 0\n",
    "        self.data[\"label_encoded\"] = self.data[\"Label\"].map({\"spam\": 1, \"ham\": 0})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx][\"Text\"]\n",
    "        label = self.data.iloc[idx][\"label_encoded\"]\n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer.encode(text, allowed_special={'<|endoftext|>'})       \n",
    "        # Truncate if too long\n",
    "        encoded = encoded[:self.max_length]\n",
    "        # Pad if too short\n",
    "        pad_len = self.max_length - len(encoded)\n",
    "        encoded += [self.pad_token_id] * pad_len\n",
    "        attn_mask = [1] * (self.max_length - pad_len) + [0] * pad_len\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(attn_mask, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "# Download the dataset zip file\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extract_path = \"sms_spam_collection\"\n",
    "data_file_path = os.path.join(extract_path, \"SMSSpamCollection\")\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    print(\"Download complete.\")\n",
    "# Unzip\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "# Read the TSV file\n",
    "df = pd.read_csv(\n",
    "    data_file_path, \n",
    "    sep=\"\\t\", \n",
    "    header=None, \n",
    "    names=[\"Label\", \"Text\"]\n",
    ")\n",
    "print(f\"Total samples loaded: {len(df)}\")\n",
    "\n",
    "# 4. Create Train/Test Split (80 train / 20 test)\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "# Split index\n",
    "split_idx = int(0.8 * len(df))\n",
    "\n",
    "# TODO: if needed (for performance resons), you can come back here and reduce the size of the training set.\n",
    "train_df = df.iloc[:split_idx]  # [:2000]  # Readd this to only consider 2000 training samples\n",
    "test_df = df.iloc[split_idx:]\n",
    "\n",
    "# Save as CSVs, so the SpamDataset class can read them.\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "print(\"Created 'train.csv' and 'test.csv' successfully!\")\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1836438",
   "metadata": {},
   "source": [
    "**Question 5.1**: In the cell above, why did we do `df = df.sample(frac=1, random_state=123)` when creating the train/test split?\n",
    "\n",
    "**Question 5.2**: Analyse the datasets, what is the distribution of the two classes in the train set? Are they balanced or unbalanced? In case they are unbalanced, might this lead to issues for the fine-tuning of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75032163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     3860\n",
      "spam     597\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['Label'].value_counts())\n",
    "class_distribution = train_df['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942400a0",
   "metadata": {},
   "source": [
    "**Question 6**: Create the dataloaders for training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cc53c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add any imports which are needed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create the Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Instantiate the Dataset\n",
    "train_dataset = SpamDataset(\"train.csv\", tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", tokenizer)\n",
    "\n",
    "# --- TODO: Create DataLoaders ---\n",
    "# 1. Create a train_loader with batch_size=16 and shuffle=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# 2. Create a test_loader with batch_size=16 and shuffle=False\n",
    "test_loader =  DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eb1bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([16, 120])\n",
      "Attention mask shape: torch.Size([16, 120])\n",
      "Target batch shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Check your work\n",
    "for input_batch, attn_mask_batch, target_batch in train_loader:\n",
    "    print(\"Input batch shape:\", input_batch.shape)        # [16, 120]\n",
    "    print(\"Attention mask shape:\", attn_mask_batch.shape)  # [16, 120]\n",
    "    print(\"Target batch shape:\", target_batch.shape)      # [16]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac363ae",
   "metadata": {},
   "source": [
    "**Question 7**: Looking at the batch size and the training size, how many batches will you have in total? Please report the size of the subsampled training data, you reduce it due to performance constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e612ab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer to Q7: Calculating the number of batches...\n",
      "The training dataset has 4457 samples.\n",
      "With a batch size of 16, there are 279 batches in the train_loader.\n",
      "Calculation: ceil(4457 / 16) = 279\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nAnswer to Q7: Calculating the number of batches...\")\n",
    "num_batches = len(train_loader)\n",
    "print(f\"The training dataset has {len(train_df)} samples.\")\n",
    "print(f\"With a batch size of 16, there are {num_batches} batches in the train_loader.\")\n",
    "print(f\"Calculation: ceil({len(train_df)} / 16) = {num_batches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92492b3f",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeab783",
   "metadata": {},
   "source": [
    "**Context**: GPT-2 was trained to predict the next word (output size ~50,000). We want to predict binary classes (output size 2), so we must replace the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964803ec",
   "metadata": {},
   "source": [
    "**Question 8**:\n",
    "\n",
    "**8.1**: In the cell below, define the number of output classes (`num_classes`) for the new spam detection task.\n",
    "\n",
    "**8.2**: Also, pring the original and updated output heads (hint: `out_head` from `GPTModel`)\n",
    "\n",
    "**8.3**: Why do we freeze the internal layers with `param.requires_grad = False`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d5a5910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output head: Linear(in_features=768, out_features=2, bias=True)\n",
      "\n",
      "Answer to Q8.3: We freeze the internal layers to leverage transfer learning. The pre-trained GPT-2 model has already learned a powerful representation of the English language. By freezing its layers, we keep this knowledge intact and only train the new classification head and fine-tune the very last layer norm. This is much more efficient than training the whole model from scratch, requires significantly less data, and helps prevent overfitting on our small dataset.\n",
      "New output head: Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Freeze the internal layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Original output head: {model.out_head}\")\n",
    "\n",
    "print(\"\\nAnswer to Q8.3: We freeze the internal layers to leverage transfer learning. The pre-trained GPT-2 model has already learned a powerful representation of the English language. By freezing its layers, we keep this knowledge intact and only train the new classification head and fine-tune the very last layer norm. This is much more efficient than training the whole model from scratch, requires significantly less data, and helps prevent overfitting on our small dataset.\")\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=model_config[\"emb_dim\"], out_features=num_classes)\n",
    "# Hint: The input size of the last layer in GPT-2 small is 768.\n",
    "\n",
    "# Enable gradient calculation ONLY for the new head and the final LayerNorm\n",
    "for param in model.out_head.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.trf_blocks[-1].norm2.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"New output head: {model.out_head}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef4e51",
   "metadata": {},
   "source": [
    "You now have to **finalise the code for the training loop** (see individual steps below).\n",
    "\n",
    "In the first cell below you can find the code to move the model to GPU (if available), define the optimizer, and calculate the accuracy. The following cell contains the code for the training (fine-tuning) loop.\n",
    "\n",
    "You will have to complete the code of the training loop, by answering the following questions:\n",
    "\n",
    "**Question 9.1**: Reset the gradients of the `optimizer` ([hint](https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html)).\n",
    "\n",
    "**Question 9.2**: Compute cross-entropy loss ([hint](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)).\n",
    "\n",
    "**Question 9.3**: Add code for the backward pass, to compute the gradient ([hint](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html))\n",
    "\n",
    "**Question 9.4**: Add code for the optimizer step, to update the weights ([hint](https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html))\n",
    "\n",
    "**Question 9.5**: Add code to calculate the accuracy on train and test (hint: you can use the `calc_accuracy` method).\n",
    "\n",
    "**Note about the speed**: On my laptop's CPU 1 epoch with the full training dataset (~4400 samples, batch_size=16) took ~20 minutes; 1 epoch with a train set of 2000 samples (batch_size=16) took ~12 minutes. \n",
    "\n",
    "To iterate more quickly, you could:\n",
    "- i) set `num_epochs = 1` (but only at the beginning), just to make sure that the code is working;\n",
    "- ii) increase batch_size to 32 or 64 (but careful with possible memory issues).\n",
    "- iii) reduce the size of the training dataset, by going back to the *Preparing the data* section, and changing the line `train_df = df.iloc[:split_idx]` to `train_df = df.iloc[:split_idx][:2000]` or similar. Be careful that if you reduce the training data too much, the model will not have enough data for fine-tuning.\n",
    "- Use a GPU; it would be much quicker (few minutes on the whole training data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c48da4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: tensor([1.0000, 2.5428])\n"
     ]
    }
   ],
   "source": [
    "# [--- INSTRUCTOR CODE ---]\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Measure imbalance\n",
    "count_ham = len(train_df[train_df['Label']=='ham'])\n",
    "count_spam = len(train_df[train_df['Label']=='spam'])\n",
    "\n",
    "# Calculate weight: penalize missing the minority class (Spam) more\n",
    "# Weight = Count(Majority) / Count(Minority)\n",
    "pos_weight = (count_ham / count_spam) ** 0.5   # sqrt ratio (less aggressive)\n",
    "class_weights = torch.tensor([1.0, pos_weight]).to(device)\n",
    "\n",
    "print(f\"Using class weights: {class_weights}\")\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "# Calculate Accuracy Helper Function\n",
    "def calc_accuracy(loader, model, device):\n",
    "    correct, total = 0, 0\n",
    "    # Track spam specifically\n",
    "    spam_correct, spam_total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, attn_mask, labels in loader:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            out = model(inputs)\n",
    "            last_idx = attn_mask.sum(dim=1) - 1\n",
    "            logits = out[torch.arange(out.size(0), device=device), last_idx, :]\n",
    "            predicted = torch.argmax(logits, dim=-1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # Filter for Spam (Label 1)\n",
    "            spam_mask = (labels == 1)\n",
    "            spam_total += spam_mask.sum().item()\n",
    "            spam_correct += (predicted[spam_mask] == labels[spam_mask]).sum().item()\n",
    "    # Avoid division by zero\n",
    "    spam_acc = spam_correct / spam_total if spam_total > 0 else 0.0\n",
    "    global_acc = correct / total\n",
    "    return global_acc, spam_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e30f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 279/279 [18:10<00:00,  3.91s/it, loss=1.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Train Acc: 87.37% (Spam Acc: 19.10%) | Test Acc: 88.34% (Spam Acc: 25.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 279/279 [09:46<00:00,  2.10s/it, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary: Train Acc: 93.02% (Spam Acc: 56.11%) | Test Acc: 93.18% (Spam Acc: 59.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 279/279 [09:44<00:00,  2.09s/it, loss=0.215] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary: Train Acc: 95.98% (Spam Acc: 77.55%) | Test Acc: 95.70% (Spam Acc: 76.67%)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # Adding a progress bar for better visualization\n",
    "    from tqdm import tqdm\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (inputs, attn_mask, targets) in progress_bar:\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # 9.1. Reset Gradients (of the `optimizer`)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass. The model outputs (batch, seq_len, vocab_size).\n",
    "        # We only want the prediction for the LAST token in the sequence.\n",
    "        out = model(inputs)  # (B, T, C)\n",
    "        last_idx = attn_mask.sum(dim=1) - 1          # (B,)\n",
    "        logits = out[torch.arange(out.size(0), device=device), last_idx, :]  # (B, C)\n",
    "\n",
    "        # 9.2. Calculate the cross entropy loss\n",
    "        loss = F.cross_entropy(logits, targets, weight=class_weights)\n",
    "\n",
    "        # 9.3. Backward Pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 9.4 Optimizer Step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # 9.5 Add code to calculate the accuracy on train and test\n",
    "    train_acc, train_spam_acc = calc_accuracy(train_loader, model, device)\n",
    "    test_acc, test_spam_acc = calc_accuracy(test_loader, model, device)\n",
    "    print(f\"\\nEpoch {epoch+1} Summary: Train Acc: {train_acc*100:.2f}% (Spam Acc: {train_spam_acc*100:.2f}%) | Test Acc: {test_acc*100:.2f}% (Spam Acc: {test_spam_acc*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cee15",
   "metadata": {},
   "source": [
    "**Question 10**: \n",
    "\n",
    "Now run the cell above. You should see how the training loss changes after each batch (and epoch).\n",
    "Describe thie trend: what do you see, is the model learning?\n",
    "\n",
    "**Question 11 (optional)**: Change the number of epochs and/or the learning rate and/or the size of the training data, and investigate how the loss/accuracy of the model changes. You can do this editing and re-running the cells above, or creating new cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3ef76",
   "metadata": {},
   "source": [
    "**Question 12 (optional)**: Now test the model *on your own text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed98ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: Congratulations! You've been selected as a lucky winner of a brand new iPhone 18 Pro Max! To claim your prize, simply click on this link: [malicious-link.xyz/claim] and enter your bank details. Don't miss out, offer valid for 24 hours  only -> SPAM\n",
      "Text 2: Hey, are we still on for dinner tonight at 7? Let me know! -> NOT SPAM\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text, model, tokenizer, device, max_length=120, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the text\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    \n",
    "    # Pad/Truncate \n",
    "    # (Matches the logic in SpamDataset so the model sees familiar input structures)\n",
    "    encoded = encoded[:max_length]\n",
    "    pad_len = max_length - len(encoded)\n",
    "    encoded += [pad_token_id] * pad_len\n",
    "\n",
    "    # Create tensor and add batch dimension\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device) # Shape: [1, max_length]\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        out = model(encoded_tensor)\n",
    "        last_idx = (encoded_tensor != pad_token_id).sum(dim=1) - 1\n",
    "        logits = out[torch.arange(out.size(0), device=device), last_idx, :]\n",
    "    # Logits for the last token\n",
    "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"SPAM\" if predicted_label == 1 else \"NOT SPAM\"\n",
    "\n",
    "# --- TODO: Test the model ---\n",
    "# Create 2 strings: one clearly spam, one normal.\n",
    "text_1 = \"Congratulations! You've been selected as a lucky winner of a brand new iPhone 18 Pro Max! To claim your prize, simply click on this link: [malicious-link.xyz/claim] and enter your bank details. Don't miss out, offer valid for 24 hours  only\"\n",
    "text_2 = \"Hey, are we still on for dinner tonight at 7? Let me know!\"\n",
    "\n",
    "print(f\"Text 1: {text_1} -> {classify_text(text_1, model, tokenizer, device)}\")\n",
    "print(f\"Text 2: {text_2} -> {classify_text(text_2, model, tokenizer, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e812afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
